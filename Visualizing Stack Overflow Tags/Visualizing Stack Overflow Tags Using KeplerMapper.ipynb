{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8bd313",
   "metadata": {},
   "source": [
    "# Visualizing Stack Overflow Tags Using KeplerMapper\n",
    "## Table of Contents\n",
    " 1. [Introduction](#Introduction)\n",
    " 2. [Implementation](#Implementation)\n",
    " 3. [Analysis](#Analysis)\n",
    " 4. [Theoretical Perspective](#Theoretical-Perspective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc96d4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the [last project](https://github.com/daniel-rossano/Data-Analysis-Projects/blob/main/Persistent%20Homology%20SO%20Tags/Persistent%20Homology%20of%20Stack%20Overflow%20Tags.ipynb), I looked at a subset of popular [Stack Overflow](https://stackoverflow.com/) tags, and tried to analyze them using persistent homology. Although the previous analysis suggested there were a specific number of \"clusters\" of tags which are often used together in the same posts, it was lacking because there was no way to determine which tags belong to which clusters, and no way to visualize the clusters.\n",
    "\n",
    "In this project I attempt to remedy the issues from the last attempt by analyzing tags through a different approach. [KeplerMapper](https://kepler-mapper.scikit-tda.org/en/latest/) is a Python library which implements the Mapper algorithm, a topological data analysis technique that combines dimensionality reduction, clustering, and graph networking to visualize high-dimensional data in a convenient and intuitive way. The idea is that, if there is a way to represent the SO tags in high-dimensional Euclidean space, one can use [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) to reduce dimensionality of the tags, then use KeplerMapper to cluster and visualize the tags. While there are too many technical details to explain here, I will briefly explain the main ideas behind each step of the process as I go through them.\n",
    "\n",
    "The motivation for this project is that, by understanding which SO tags are clustered together, one can recommend new, relevant tags to users if they've interracted or created posts having tags in the same cluster. Such a recommendation system can be used to increase user engagement.\n",
    "\n",
    "Also, if you are not doing so already, it is recommended to view this project in [NBViewer](https://nbviewer.org/github/daniel-rossano/Data-Analysis-Projects/blob/main/Visualizing%20Stack%20Overflow%20Tags/Visualizing%20Stack%20Overflow%20Tags%20Using%20KeplerMapper.ipynb), so that one can easily see and interact with the output. That is the only cell that needs to be run if one wishes to see the visualization of tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ca0a0",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "The implementation of this project is surprisingly simple; it is the meaning behind the code that requires more explanation. For our first step, I want to represent the SO tags in Euclidean space. In theory, I could simply represent the tags in two or three dimensions and skip the PCA, but it is difficult to find two or three distinct yet meaningful ways to quantify the SO tags. Instead, I can represent the tags in high-dimensional space by describing tags in terms of their usages and co-occurrences with other tags. For example, if there were only three tags to analyze, I would represent each of them in $\\mathbb{R}^3$ by:\n",
    "\n",
    "$$\\text{tag}_1 = (\\text{usages of tag}_1, \\text{co-occurrences of tag}_1 \\text{ and tag}_2, \\text{co-occurrences of tag}_1 \\text{ and tag}_3)$$\n",
    "$$\\text{tag}_2 = (\\text{co-occurrences of tag}_2 \\text{ and tag}_1, \\text{usages of tag}_2, \\text{co-occurrences of tag}_2 \\text{ and tag}_3)$$ \n",
    "$$\\text{tag}_3 = (\\text{co-occurrences of tag}_3 \\text{ and tag}_1, \\text{co-occurrences of tag}_3 \\text{ and tag}_2, \\text{usages of tag}_3).$$ \n",
    "\n",
    "This project must therefore be restricted to a random sample of $50,000$ posts, because SO does not archive co-occurrences of tags across the whole site, and I need to calculate them manually. The project is restricted specifically to $50,000$ posts because that is the limit to how many outputs one can get using the [Stack Exchange Data Explorer](https://data.stackexchange.com/stackoverflow/query/new) to obtain this sample. This contrasts with [StackAPI](https://stackapi.readthedocs.io/en/latest/) which, despite its convenience, limits queries to $10,000$ per day. \n",
    "\n",
    "To make the best use of this analysis, I will filter out a lot of unpopular tags so that they do not cloud up the visualization or waste computation time. Just as I did last time, I will start with a sample of $50,000$ random posts to study tags from, and then only consider tags that have been used in *at least* $100$ posts, i.e., $0.02\\%$ of the posts in the sample. As I've learned new things in Python, I've also found a more efficient way to do this than last time. For starters, it is not necessary look at the $50,000$ most popular tags and cross-reference with their usages over the posts. By querying the Data Explorer to report back only the tags used in the random selection of $50,000$ posts, I can just read in the sample using pandas and split the string of tags accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c8acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "posts = pd.read_csv(\"SampleofPosts.csv\")\n",
    "\n",
    "all_tags = {} #used to store usages of the tags so it will be easier to sieve out tags used in less than 100 posts\n",
    "relevant_tags = {} #used to store the usages of only the relevant tags, i.e., only those used in >= 100 posts\n",
    "count_of_pairs = {} #used to store all co-occurrences of relevant tags\n",
    "\n",
    "\n",
    "#iterate through each row, and use re.findall with the given pattern to split a given row into a list of tags\n",
    "for tags in posts[\"Tags\"]:\n",
    "    tag_list = re.findall(r'<(.*?)>', tags)\n",
    "    \n",
    "    #go through each tag in the tag list\n",
    "    for tag in tag_list:\n",
    "        if tag in all_tags: #if already accounted for the tag in question, increment its usage count\n",
    "            all_tags[tag] += 1\n",
    "        else:\n",
    "            all_tags[tag] = 1 #else, this tag is being stored for the first time\n",
    "\n",
    "\n",
    "\n",
    "#search through all_tags, and store only the tags used >=100 times in the sample\n",
    "for tag, count in all_tags.items():\n",
    "    if count >= 100:\n",
    "        relevant_tags[tag] = count\n",
    "\n",
    "\n",
    "#and now I might as well set all_tags to None since I don't need it anymore\n",
    "all_tags = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8dce062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations #helps us efficiently iterate through combinations of tags\n",
    "\n",
    "tag_pairs = {} #used to store pairs of tags as keys and their co-occurrences as values\n",
    "\n",
    "#have to check co-occurrences by reading in tags for each post, just as before\n",
    "for tags in posts[\"Tags\"]:\n",
    "    tag_list = re.findall(r'<(.*?)>', tags)\n",
    "    \n",
    "    #for all unique pairs of tags in the tag list, if tag1 and tag2 are relevant tags...\n",
    "    for tag1, tag2 in combinations(tag_list, 2):\n",
    "        if tag1 in relevant_tags and tag2 in relevant_tags:\n",
    "            \n",
    "            #...check if tag1 is lexicographically larger than tag2 to weed out redundant computations\n",
    "            if tag1 > tag2: \n",
    "                tag1, tag2 = tag2, tag1\n",
    "                \n",
    "            #and create a tuple of the two tags to act as a key, then proceed as before\n",
    "            pairing = (tag1, tag2) \n",
    "            if pairing in tag_pairs:\n",
    "                tag_pairs[pairing] += 1\n",
    "            else:\n",
    "                tag_pairs[pairing] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9d3b0",
   "metadata": {},
   "source": [
    "Now that I've found and stored all the tag usages and co-occurrences, I can start analyzing the tags. A quick check: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4233ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "print(len(relevant_tags.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4eb572",
   "metadata": {},
   "source": [
    "shows that there are $187$ tags to work with, and therefore each tag will be represented as a tuple in $\\mathbb{R}^{187}$. Visualizing the tags right now is, of course, not possible for anyone living in less than $187$ dimensions. So naturally the next step is to reduce dimension. For this project, I decided on using PCA because\n",
    "1. it maximizes variance and is based on linear transformations, so it will be better at preserving structure than say, t-SNE or UMAP\n",
    "2. it is easy to implement thanks to libraries like sklearn\n",
    "\n",
    "The basic idea is that I'll run sklearn's `PCA`  on the tag-tuples, reducing it to a three-dimensional space by keeping only the three most important components in each tuple, and then project into this three-dimensional space using KeplerMapper.\n",
    "\n",
    "To use `PCA`, I will use `relevant_tags` and `tag_pairs` to create a 2D numpy array, where each row is represented by one of the tag-tuples. This can be thought of as a $187 \\times 187$ matrix, where entry $i,j$ is the usage/co-occurrence of $\\text{tag}_i$ with $\\text{tag}_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c10e0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#use a variable to list all the relevant tags; will need this later to label members of clusters, anyway\n",
    "tag_list = list(relevant_tags.keys())\n",
    "#initialize the 2D array as n x n\n",
    "tag_matrix = np.zeros((len(tag_list), len(tag_list)))\n",
    "\n",
    "#this will be less painful if indices of the matrix are tracked\n",
    "#so create a simple mapping of tags to sorted indices in the tag list\n",
    "index_map = {tag: index for index, tag in enumerate(tag_list)}\n",
    "\n",
    "#now easily fill in the diagonals\n",
    "for tag, count in relevant_tags.items():\n",
    "    i = index_map[tag] #find the index of the tag to put the count in the right entry\n",
    "    tag_matrix[i,i] = count\n",
    "\n",
    "#fill in the non-diagonal entries analogously, keeping in mind this is a symmetric matrix\n",
    "for (tag_A, tag_B), count in tag_pairs.items():\n",
    "    i = index_map[tag_A]\n",
    "    j = index_map[tag_B]\n",
    "    tag_matrix[i,j] = count\n",
    "    tag_matrix[j,i] = count\n",
    "\n",
    "#no need for index_map anymore\n",
    "index_map = None\n",
    "\n",
    "#initialize sklearn's PCA, remembering to keep 3 principal components\n",
    "pca = PCA(n_components = 3)\n",
    "#now fit_transform the tag matrix and store the resulting array\n",
    "tag_pca = pca.fit_transform(tag_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e0783",
   "metadata": {},
   "source": [
    "One last thing to consider is the choice of clustering algorithm. The Mapper algorithm works by projecting a high-dimensional data set into a low-dimensional data set, choosing a cover for the low-dimensional data set (KM only allows for cubes at this point in time), and then clustering points locally based on their location in the cover. Next the Mapper algorithm constructs a graph where each node represents a cluster, and whenever a point lies inside more than one cluster (which may happen due to overlap among cubes), an edge will be drawn between the corresponding nodes. For the record, KM allows for *any* projection function, and *any* clustering algorithm or distance metric. I chose to use k-means clustering for this as that seems to be the standard for text clustering. Fortunately, sklearn also provides an algorithm for k-means clustering. \n",
    "\n",
    "I will initialize the graph formed by the Mapper algorithm in one line. To make it is easy to track which tags belong to which cluster(s), I will form a numpy array of all the tags and use this array in the `custom_tooltips` parameter, so that KM labels the tags in the correct order. I will also separate the tags by square brackets so they are easier to read on the graph. Finally, I will use `KeplerMapper.visualize` to output a .html file of the graph and begin the analysis. Because this project is written in Jupyter Notebook, I must take the extra step of importing KM's `jupyter` to create a custom display of the .html file using the notebook.\n",
    "\n",
    "If you are interested in exploring the KM output yourself, simply download this notebook (or use Binder) and run, being sure to uncomment the first `IFrame` line. If you are viewing this project on GitHub, it is recommended [you use NBViewer](https://nbviewer.org/github/daniel-rossano/Data-Analysis-Projects/blob/main/Visualizing%20Stack%20Overflow%20Tags/Visualizing%20Stack%20Overflow%20Tags%20Using%20KeplerMapper.ipynb) so you can easily run the notebook and view the .html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fe2a19c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"Visualization-of-SO-Tags.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fad15155b20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import kmapper as km\n",
    "\n",
    "\n",
    "#initialize mapper\n",
    "mapper = km.KeplerMapper()\n",
    "#and now build the graph; cluster using specified # of kmeans clusters; build cover w/ % overlap\n",
    "graph = mapper.map(tag_pca, tag_matrix, clusterer = sklearn.cluster.KMeans(n_clusters = 6), \n",
    "                   cover = km.Cover(n_cubes = 20, perc_overlap = 0.3))\n",
    "\n",
    "#to construct the array just modify the existing tag list and convert to np array\n",
    "#the conditional is here to prevent adding multiple sets of brackets upon re-running this cell\n",
    "if tag_list[0].find('[') == -1:\n",
    "    tag_list = ['[' + tag + ']' for tag in tag_list]\n",
    "labels = np.array(tag_list)\n",
    "#finally, visualize and output the graph\n",
    "mapper.visualize(graph, path_html=\"Visualization-of-SO-Tags.html\", title = 'Clusters of Stack Overflow Tags',\n",
    "                custom_tooltips = labels)\n",
    "\n",
    "#for local use:\n",
    "from IPython.display import IFrame\n",
    "IFrame(src = \"Visualization-of-SO-Tags.html\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf52db3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://nbviewer.org/github/daniel-rossano/Data-Analysis-Projects/blob/main/Visualizing%20Stack%20Overflow%20Tags/Sample%20and%20Visualization/Visualization-of-SO-Tags.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fad15155e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this cell to view output; no need to run any other cells unless you are curious and have data experiment with!\n",
    "from IPython.display import IFrame #used to display the .html in the notebook environment\n",
    "IFrame(src =\"https://nbviewer.org/github/daniel-rossano/Data-Analysis-Projects/blob/main/Visualizing%20Stack%20Overflow%20Tags/Sample%20and%20Visualization/Visualization-of-SO-Tags.html\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454e713",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "With the output displayed, all that remains is to ask, \"what does it tell us about the Stack Overflow tags?\" For starters, one should keep in mind that there are *many* different ways to change the simplicial complex generated by KM: projection function, filtering function and its number of clusters or definition of closeness, number of cubes, and percentage of cube overlap can all dramatically change the output with even the slightest adjustment. The choices of projection and filtering function were already justified, though they may not be perfect! As for why I used $6$ k-means clusters and $20$ cubes with $30\\%$ overlap, it was essentially the result of trial-and-error, with these parameters yielding the most interesting results.\n",
    "\n",
    "In short, I believe that the current output does suggest a few relationships among SO tags, however there are likely other methods of analyzing these tags. Unfortunately there are some \"mega nodes\" which contain such a vast majority of tags that they cannot possibly be described by one concept, programming language, or computer science discipline. Attempts to \"break up\" these clusters into smaller, more descriptive clusters by changing the parameters mentioned above did not seem any more productive. Additionally, there are tags that one would expect to be incredibly popular, like \"C++,\" that appear to be isolated from every other tag, which suggests not all important features of the data were captured in this process. If one wishes to improve upon the analysis here, it should be noted that it is possible  PCA is not the optimal method of dimensionality reduction for this data set. Or perhaps another, non-linear clustering algorithm like DBSCAN or t-SNE may be more useful for further analysis.\n",
    "\n",
    "On a more positive note, in between the \"isolated nodes\" and \"mega nodes,\" there are some smaller nodes having $2-25$ members with commonalities. Of particular interest may be the \"web-dev node\" of $25$ members, containing tags such as \"xml,\" \"sqlite,\" and \"android-studio,\" as the majority of its entries refer directly to technologies, environments, and protocols used in web development. This finding seems to suggest that the work done here is not in vain, but rather in need of further exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f7555",
   "metadata": {},
   "source": [
    "# Theoretical Perspective\n",
    "This section is best suited for the mathematically inclined, and involves only theoretical ideas. The analysis above serves as the true summary of this project. But one may wonder how well the analysis of tags using KeplerMapper holds up against the [analysis using persistent homology](https://github.com/daniel-rossano/Data-Analysis-Projects/blob/main/Persistent%20Homology%20SO%20Tags/Persistent%20Homology%20of%20Stack%20Overflow%20Tags.ipynb) (PH). Although the overall number of clusters differs quite a bit from the PH approach, this is mostly due to the choice of deciding which clusters counted as sufficiently persistent. In theory, both approaches are actually quite similar; understanding why requires some mathematical understanding of what is happening in each project.\n",
    "\n",
    "For the PH approach, I clustered tags by defining an abstract finite metric space $M = (X,d)$, where $X$ is a set whose elements are the set $P$ of all posts in the sample which are tagged with some tag $T$, and $d$ is the Jaccard metric, defined by $d(P,Q) = 1 - \\frac{|P \\cap Q|}{|P \\cup Q|}$ (here $| \\cdot |$ denotes the cardinality of a set).\n",
    "\n",
    "In the analysis using KeplerMapper, I embedded the tags into $\\mathbb{R}^n$ for $n = |X|$, using a finite set $Y$, whose elements are tuples are of the form $y_i = (|X_i \\cap X_1|, |X_i \\cap X_2|, \\ldots , |X_i \\cap X_n|)$, and where $X_i$ is a fixed element of $X$ and $X_1,X_2,\\ldots,X_n$ are the $n$ elements of $X$. Any clustering algorithm will apply the Euclidean metric $| \\cdot |$ on $Y$, and therefore the difference between the two projects is that I've gone from working with the abstract metric space $M$ to working with the Euclidean metric space $N = (Y, | \\cdot |)$. In particular, this means the results will be similar if there is some way to preserve the notion of \"closeness\" between elements of $M$ and elements of $N$. One can formalize the setup for this problem mathematically by the following:\n",
    "\n",
    "Let $X = \\{X_i\\}^{i=n} _{i=1}$ be a finite collection of nonempty finite sets, and let $M = (X, d)$ be the finite metric space formed by equipping $X$ with the metric $d(X_i, X_j) = 1 - \\frac{|X_i \\cap X_j|}{|X_i \\cup X_j|}$. Now let $Y \\subset \\mathbb{R}^n$ be a finite set consisting of all tuples of the form $y_i = (|X_i \\cap X_1|, |X_i \\cap X_2|, \\ldots , |X_i \\cap X_n|)$ for each $1 \\leq i \\leq n$. \n",
    "\n",
    "Ideally, I would like to find a continuous function $f:M \\to N$ that can preserve certain properties of the clusters. Truly expanding on this idea and proving it are certainly items of interest, but this will need to be done at some point in the future when I have more time to do so."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
